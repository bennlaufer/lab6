# lab6

Both approaches—whether using a bank of 16 small CNNs feeding a Random Forest or a single end-to-end CNN—ended up getting 60% of the 20 test images correct. In both pipelines, “Kelly” images were slightly easier to identify (recall ~70%) than “Alex” images (~56%), which isn’t too surprising given their more distinctive visual patterns in our dataset. The glaring gap, however, is that “Hunter” never showed up in our final 3-class predictions, even though we intentionally added seven Hunter-style photos into the test set. In short, the models simply ignored Hunter because they had almost no Hunter examples to learn from compared to roughly 250 Alex and 250 Kelly images.

Because Alex and Kelly each contributed an order of magnitude more training samples than Hunter, both the feature-cascade meta-classifier and the direct CNN effectively learned to treat “Hunter” as a negligible third class. Even after injecting seven Hunter-like photos into the test set, that tiny handful could not counterbalance the 500 other images. In a Random Forest, the meta-features for Hunter rarely emerged during training, so the model never really “understood” how to map any image to Hunter. Similarly, the end-to-end CNN simply matched every novel image to Alex or Kelly because, statistically, those classes dominated the loss function.

Because our test set included only seven Hunter‐style photos versus roughly 250 Alex and 250 Kelly photos in training, Hunter never appeared in any predicted labels under both pipelines. Even though we injected Hunter‐like examples into the test set, the models could not overcome the severe class imbalance. In practice, the Random Forest in Approach 1 scarcely saw any true “Hunter” feature vectors, so it defaulted to Alex/Kelly whenever uncertainty arose. Likewise, the single CNN in Approach 2 learned to map almost everything to Alex or Kelly because those classes dominated its loss function.

By using Approach 2, we avoided the overhead of training and maintaining sixteen CNNs plus a meta‐classifier—yet still matched the 60% test accuracy of the more complex setup. If we truly wanted Hunter to register as a balanced third class, we’d need to add hundreds more labeled Hunter images or apply aggressive oversampling/class weighting. Without those adjustments, both pipelines simply treated Hunter as a negligible outlier. In short: we chose the end-to-end CNN for its simplicity (one model instead of seventeen), its identical 60% test accuracy, and because there was no practical way—given our small Hunter sample—to force Approach 1’s Random Forest to learn a distinct Hunter decision boundary.
